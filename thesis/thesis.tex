\documentclass[hyperref, beleg, german]{cgvpub}
% other document types next to bachelorofscience:
% masterofscience
% diplominf
% diplomist
% beleg

%more options (to be appended in the square brackets):
% german....... german version 
% female........ to be used for female endings in german
% bibnum....... numerical reference style
% final............ intended for the final submission
% lof.............. genereate list of figures
% lot.............. generate list of tables
% noproblem.. do not show task
% notoc......... do not generate table of contents
% twoside...... two sided layout

%\usepackage[ngerman]{babel}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{tikz-3dplot}

\author{Mario Henze}
\title{The title of the thesis}
\birthday{19. July 1994}
\placeofbirth{Wernigerode}
\matno{4039602}
\betreuer{Dr.\ B.\ Russig}
\bibfiles{literature}
\problem{
VR environments become more and more important in visualization and engineering
applications. In order to provide a comfortable experience without risk of
motion sickness a rendering framerate of 90fps needs to be ensured. Framerate is
even more important than image quality. The goal of this thesis is the
development of an incremental rendering approach for large point cloud data that
can guarantee the necessary framerate by rendering only a subset of the points
per frame and by accumulation information over several frames.

The specific tasks are:
\begin{itemize}
\item Literature research on point cloud rendering and incremental rendering
	approaches
\item Justified selection of an incremental rendering approach for basic point
	cloud rendering that guarantees a user-defined framerate and converges
	to a good approximation of a non-incremental rendering without framerate
	limitations.
\item Implementation of the selected approach in a plugin to the CGV framework
\item Evaluation of the achieved image quality for stereoscopic rendering at
	90fps with a suitable image quality metric for several point clouds of
	different sizes in comparison to a non-incremental rendering approach.
\end{itemize}

Optional Tasks:
\begin{itemize}
\item Optimization of the rendering approach for stereoscopic render
\item Implementation of an advanced hole-free point cloud rendering approach
\end{itemize}
}
\copyrighterklaerung{If the author used ressources from third parties (texts,
images, code) he or she should state the consents of the copyright owners here
or cite the given general conditions (e.g. CC/ (L) GPL/BSD copyright notices)}
\acknowledgments{I'd like to thank...}
\abstracten{abstract text english}
\abstractde{ abstract text german}
\begin{document}

\section{Literaturübersicht}

Dieser Abschnitt dient zunächst der Auflistung potentieller Quellen und wird in
der finalen Fassung nicht vorkommen.

\begin{description}
	\item[\cite{discher2018point}]
	\item[\cite{rusinkiewicz2000qsplat}]
	\item[\cite{goswami2010high}]
	\item[\cite{wimmer2006instant}]
	\item[\cite{shum2000review}]
	\item[\cite{mark1997post}]
	\item[\cite{mcmillan1995head}]
	\item[\cite{chang1999ldi}]
	\item[\cite{he1998layered}]
	\item[\cite{mcmillan2009image}]
\end{description}

\chapter{Einleitung}%
\label{sec:einleitung}

Der Prozess der Punktwolkenakquisition ist durch technologische Fortschritte
mittlerweile für eine große Anzahl an Problemstellungen machbar und
kostengünstig. So sind bereits Geräte wie die Xbox Kinect oder Intel
Realsense, die selbst für Privatpersonen erschwinglich sind, in der Lage,
Tiefeninformationen zu erfassen und letztendlich Punktwolken zu generieren. Mit
dem Aufkommen von Head Mounted Displays wie der Oculus Rift oder HTC Vive
existieren nun auch Darstellungsmöglichkeiten, die die gewonnen
Tiefeninformationen der menschlichen Wahrnehmung direkter und intuitiver
zugänglich machen.

Die Bildsynthese und Wiedergabe in solchen HMD Systemen stellt jedoch besondere
Vorgaben in Hinblick auf Benutzbarkeit. So muss jederzeit sichergestellt werden,
dass die Immersion dieser Virtual Reality Umgebung erhalten bleibt. Das bedeutet
es herschen harte Schranken für Eingabeverzögerung und Bildwiederholfrequenz und
deren Übertretung hat den Verlust von Interaktivität und im schlimmsten Fall
physiologische Probleme wie Motion Sickness zur Folge.

In dieser Arbeit soll ein Verfahren zur interaktiven Darstellung von Punktwolken
entwickelt werden, welches in Hinblick auf die genannten Probleme optimiert ist.
So besteht das Ziel darin, selbst größte Datensätze noch in interaktiver Weise
darzustellen. Dazu werden zunächst Ansätze zur klassischen Punktwolken
Darstellung und zum inkrementellen Rendering vorgestellt und deren
Kombinationsmöglichkeiten untersucht. In einem weiteren Schritt soll, das so
entwickelte Verfahren als C++ Plugin für das cgv-Framework des Lehrstuhls
umgesetzt werden.

\chapter{Verwandte Arbeiten}%
\label{sec:verwandte_arbeiten}

Im Bereich des Image-based Rendering kann die Arbeit~\cite{shum2000review} als
Überblick gesehen werden. Heung-Yeung und Sing Bing arbeiten hierbei die
unterschiedliche Verwendung von geometrischen Information bei verschiedenen
Bildsyntheseverfahren heraus. So wird zunächst eine plenoptische Funktion
definiert, welche als Grundlage der geometrielosen Verfahren genutzt wird. In
der Mitte des Spektrums sind Methoden mit impliziter Geometrieinformation
aufgeführt. In diese Kategorie fallen View-morphing und Transfermethoden, welche
die geometrische Redundanz zweier aufeinanderfolgender Bilder ausnutzen. Zuletzt
werden auch die klassischen Verfahren der Bildsynthese, welche auf explizite
Geometrie wie Dreiecksnetze aufsetzen, eingeordnet.

In~\cite{chang1999ldi} stellen Chun-Fa et al.\ basierend auf~\cite{he1998layered}
eine Datenstruktur vor, welche zur Organisation von Referenzbildern mit
variierender Tiefenauflösung genutzt werden können. So werden Bilder aus
verschieden Perspektiven in einen sog. Layered Depth Tree eingfügt. Da bei
gleicher Auflösung Referenzbilder aus einer näheren Perspektive
Tiefeninformationen höher abtasten als entferntere, können diese in einen Octree
eingeordnet werden und es entsteht eine Art Ordnungsrelation. Mithilfe dieser
Datenstruktur ist es nun möglich, die Referenzbilder auswählen, deren
Tiefenauflösung in etwa dem zu synthetisierenden Bildausschnitts entspricht. In
der praktischen Umsetzung zeigen Chun-Fa et al.\ auf, dass der Speicherverbrauch
nicht wesentlich höher als die direkte Speicherung aller Referenzbilder ist und
auch die Rechenzeit im schlimmsten Fall gleich dem direkten Rendern aller
Referenzbilder ist.

\section{Plenoptic Modelling}

Um neuartige Renderansätze zu entwicklen, ist es hilfreich diese in nach einer
Top-Down Methodik zu entwickeln. Dafür bietet sich die plenoptische Modellierung
an. Hierbei wird zunächst versucht, die Szene unabhängig von Beobachtern zu
beschreiben. Adelson und Bergen definieren dafür in~\cite{adelson1991plenoptic}
die plenoptische Funktion wie folgt:

\begin{equation}
	P_7 = P(V_x, V_y, V_z, \theta, \Phi, \lambda, t)
\end{equation}

\begin{itemize}
	\item[\( V_x, V_y, V_z \)] mögliche Position der Kamera
	\item[\( \theta, \Phi \)] Winkel eines Sichtstrahls
	\item[\( \lambda \)] Wellenlänge des Lichts
	\item[\( t \)] Zeitpunkt der Beobachtung
\end{itemize}

Diese Form der Beschreibung ermöglicht es die Szene zu jedem Zeitpunkt an jeder
Stelle und in jede Richtung zu samplen. Anhand dieses generellen Modells können
nun Vereinfachungen vorgenommen werden.

Es ist zunächst festzustellen, dass Punktwolkendaten in einem globalen
Koordinatensystem gegeben sind. Beim Punktwolkenrendering gehen jedoch nur die
sichtbaren Punkte in das Bild ein. Außerdem erfolgt die Kamerabewegung von HMDs
ausschließlich kontinuierlich. Daraus folgt, dass sich die Untermengen alles
sichtbaren Punkte zwischen zwei gerenderten Ansichten nahezu identisch sind.
Ähnliche Annahmen liegen auch dem Modell der Layered Depth Images zugrunde.

McMillan und Bishop ermöglichten in~\cite{mcmillan1995plenoptic} unter
Ausschluss von \( t \) und \( \lambda \) das plenoptische Modellieren. In
diesem Kontext kann ein gewöhnliches Bild nun als unvollständige Abtastung der
plenoptischen Funktion an einem festen Betrachtungspunkt aufgefasst werden.

\chapter{Layered Depth Image}

\begin{itemize}
	\item Von kamera position
	\item x- und y-Auflösung als raytracing
	\item layered depth pixel ist Farbwert mit Tiefeninformation in Abhängigkeit der
	      Kameraposition
	\item neuer Viewport über Matrixtransformation
\end{itemize}

\chapter{Konzeption}

Die Entkopplung und Nebenläufigkeit der Abläufe in der Implementation sind von
großer Wichtigkeit. So können folgende unabhängige Prozesse definiert werden.

Zunächst ist eine Datenverarbeitung notwendig. In Abhängigkeit des
Datenformates und der Größe der Punktwolkendatei müssen die Punkte als Ganzes
oder als Teilmenge im Arbeitsspeicher vorgehalten werden. Des Weiteren sollten
initiale Koordinatentransformation in diesem Prozess durchgeführt werden.

Einen weiteren Prozess stellt das Rendering der Punktwolke dar. Um eine
konstante Framerate zu erreichen muss der Arbeitsaufwand möglichst konstant
gehalten werden. Deshalb werden die Punkte in einem LDI vorgehalten. Diese
Datenstruktur ermöglicht eine triviale Rasterrisierung und die Anwendung der
Morphing Equation.

Da bei großen Datenwolken Speicherknappheit auftreten würde, muss eine
Selektion aus dem gesamten Datensatz geschehen. Unter der Annahme, dass nur ein
Teil der Punkte im Sichtfeld des Betrachters für eine qualitative
Repräsentation der Daten ausreicht kann der letzte Prozess deklariert werden.
Hierfür wird das LDI des Renderingprozesses als der Kegelstumpf der
Betrachtungstransformation interpretiert. Die Vereinigung dieses Kegelstumpfes
und dem Koordinatenraum der Punktwolke schränkt die Menge der zu verarbeitenden
Punkte initial ein. Da das LDI lediglich Daten in Bezug auf den Betrachter
speichert, würden alle außerhalb liegenden Punkte nicht in der Rasterisierung
auftreten und können somit verworfen werden. Wenn sich die
Betrachtungstransformation geändert hat, muss außerdem ein neues LDI erzeugt
werden. Unter der Vorraussetzung, dass die Differenz der beiden Betrachtungen
möglichst gering sind, können die Punkte des alten LDIs übernommen werden.
Hierbei besitzen zwei Kegelstümpfe eine geringen Unterschied, wenn sich deren
vereinigtes Volumen nicht stark vom Volumen eines einzigen Keglstumpfes
unterscheidet. Eine Translation um ein Viertel der Bildschirmbreite oder eine
Rotation entlang der Blickrichtungsachse sind Beispiele für einen geringen
Unterschied zwischen zwei Betrachtern. In diesem Fall ist das Morphing mit
seinem geringen linearen Aufwand besonders lohnenswert, da bestenfalls der
Großteil der neuen Perspektive mit bereits transformierten Daten abgedeckt
werden kann. Es werden sich aber immer Stellen im LDI ergeben, an denen keine
Daten vorhanden sind. Diese Ausschnitte müssen deshalb erkannt und mit Daten
der ursprünglichen Punktwolke gefüllt werden. Hierfür muss nach dem Morphen die
lokale Punktdichte im LDI ermittelt werden. Ziel dieser Berechnung ist es
möglichst zusammenhängende Pixelbereiche zu erkennen. Sind diese Bereiche
können diese als Anfrage an den ursprünglichen Datensatz gestellt werden.

Durch die oben beschriebene Trennung ist eine Implementation möglich, welche
aus Sicht des Renderings besonders gleichmäßige Frametimes ermöglicht. So
stellt die Rasterisierung der Punkte im Sichfeld den einzigen zeitkritischen
Pfad im System dar, welcher direkt auf die Qualität der Darstellung Einfluss
nimmt. Die Datenverarbeitung der ursprünglichen Punktwolke in ein geeignetes
Format und die Bearbeitung der LDIs sind sekundäre Faktoren. Das heißt die Rate
mit der neue Punkte nachgeladen werden können, verbessert die Detailwiedergabe,
ist für die prinzipielle Funktion jedoch beliebig.

\chapter{Implementation}

Die vorliegende Arbeit implementiert den oben beschriebenen Ansatz eines LDIs.
Als Grundlage für die Benutzeroberfläche und die Interaktion mit der
Grafikhardware wird das CGV Framework des Lehrstuhls verwendet. Um möglichst
viele Punktwolkenformate als Eingabe nutzen zu können wird weiterhin die
bestehende Klasse \texttt{point\_cloud} genutzt.

\section{Grundlegende Abstraktionen}

\begin{figure}
	\centering
	\tdplotsetmaincoords{70}{110}
	\input{figures/pinholecamera.tikz}
	\caption{Abbildungsverhältnisse einer Lochkamera auf Sensor- und reale
		Bildebene}%
	\label{fig:pinholecamera}
\end{figure}

Da LDIs prinzipbedingt auf den Betrachter bezogen definiert sind, ist die
Klassendeklaration \texttt{PinholeCameraModel} nützlich. Hierbei wird die in
der Literatur häufig getroffene Vereinfachung einer Lochkameratransformation
nachgebildet. So kann die Lochkamera als idealisierte optische Kamera
aufgefasst werden, welche alle Sichtstrahlen in aus der Szene in einem einzigen
Punkt bündelt (siehe Abbildung~\ref{fig:pinholecamera}). Bei praktischen
Aufbauten eine Lochkamera wird das projizierte Bild hinter diesem Brennpunkt
gespiegelt auf einer Ebene abgebildet. Zur Vereinfachung wird auf diesen
Umstand verzichtet und die Bildebene kurz vor dem Brennpunkt zwischen Szene und
Koordinatenursprung der Kamera platziert. Dadurch wird auch die Spieglung der
Abbildung verhindert und die geometrischen Zusammenhänge können mit einfachen
Strahlensätzen interpretiert werden. In Anlehnung an digitale Linsenkameras
wird diese Ebene im Quellcode auch als Sensorebene bezeichnet. Des Weiteren
definiert die Lochkamera mit ihrem Öffnungswinkel und ihrem Seitenverhältnis
einen Sichtkegelstumpf. Die \texttt{PinholeCameraModel} Klasse vereint diese
beiden Anwendungen, indem es die Kamera- und Perspektivtransformation als
Membervariable speichert und in geeigneter Weise über Zugriffmethoden bereit
stellt. So kann der Sichtkegelstumpf eines LDIs zur Sichtbarkeitsprüfung von
Punkten genutzt werden. Durch die Rasterisierung der Punkte im LDI ist es
außerdem notwendig die Pixelauflösung der Sensorebene und damit auch der
Lochkamera zu speichern. Dies geschieht durch eine weitere Membervariable vom
Typ \texttt{std::pair<size\_t, size\_t>}.

\section{Die Punktwolke und Punktwolkenanfragen}

Die Punktwolke wird durch die \texttt{PointCloudSource} abstrahiert. Die
konkreten Punktdaten werden als Membervariable vom Typ \texttt{point\_cloud}
gekapselt. Durch diesen ist es möglich getrennt auf Koordinaten und andere
Punktmerkmale wie Farbe oder Normalen zuzugreifen.

Außerdem verwaltet die \texttt{PointCloudSource} eine Liste aller gestellten
\texttt{PointCloudQuery}s in einer weiteren Membervariable. Diese Anfragen
speichern als Member zunächst die Kamerakonfiguration vom Typ
\texttt{PinholeCameraModel}. Mit dieser ist es möglich, die Punkte der gesamten
Punktwolke nach Sichtbarkeit zu filtern. Die \texttt{PointCloudQuery} besitzt
zwei eigene Membervariablen zum Speichern der Punktpositionen und Farben. Nach
der Sichtbarkeitsprüfung werden alle verbliebenen Punkte deshalb in die Anfrage
kopiert. Da die Anfrageklasse außerdem den Schnittpunkt zwischen
Dateiverarbeitung und Rendering darstellt ist eine weitere Membervariable zur
Sicherung von kritischen Abschnitten notwendig. Die Mutexvariablen vom Typ
\texttt{std::timed\_mutex} besitzen die Besonderheit eine vorher definierte
Wartezeit zu berücksichtigen. Dies ist aus Sicht des Renderings notwendig um
eine Obergrenze für die Frametimes einhalten zu können. 

Um den Verwaltungsaufwand gering zu halten, durlaufen alle Anfragen drei
Zustände entsprechend ihrer Bearbeitung. Anfangs befinden sich die Anfragen in
einem ``unvollständigen'' Zustand. Dieser bedeutet, dass noch nicht alle Punkte
aus der Datenverarbeitung vorliegen. Nach dem vollständigen Kopieren aller
sichtbaren Punkte geht die Anfrage in einem ``vollständigen'' und
``konsumierbarem'' Zustand über. Daraufhin filtert der Renderingprozess die
Liste aller Anfragen nach der Konsumierbarkeit und fügt die entsprechenden
Punkte ein. So wird während des Renderings vesucht im vorgegebenen Zeitbudget
so viele Punkte wie möglich in das LDI zu transformieren. Anschließend ist der
Zweck der Anfrage erfüllt und sie kann aus der Liste entfernt werden. Um
Wettlauf-Risiken bei den obigen Zustandsänderungen auszuschließen, werden zwei
Membervariablen des Typs \texttt{std::atomic\_bool} verwendet. Deren Umsetzung
garantiert auf x86 basierten Computern einen wohldefinierten Zustand zu allen
Zeitpunkten der nebenläufigen Benutzung. Außerdem kann hierdurch auf
laufzeittechnisch teure Mutexvariablen und deren fehleranfällige Verwaltung
verzichtet werden.

\section{Das Layered Depth Image}

Das Layered Depth Image stellt die zentrale Datenstruktur der Implementation
dar. Sie kann als eine um einen Datenspeicher erweiterte Lochkamera
interpretiert werden. So ist bereits die \texttt{PinholeCameraModel} Klasse mit
rasterisierten Koordinaten ausgestattet. Da diese Pixelauflösung die Grenze der
Positionsauflösung auf der Sensorebene darstellt, ergeben sich folgende
Festlegungen. Nach der entsprechenden Perspektivtransformation befinden sich
alle sichtbaren Punkte an einer diskreten \( (x, y) \) Position auf der
Sensorebene. Zusätzlich ist deren Tiefe in Bezug auf die Kamera von Bedeutung.
Werden räumlich verschiedene Punkte auf die selbe \( (x,y) \) Position
transformiert, unterscheidet sich deren Tiefenwert \(z\). Konventionelle
Rasterrisierungsansätze würden bei diesem Ansatz den Tiefenpuffer zur
Konfliktauflösung verwenden und den der Kamera näheren Punkt in das Bild
zeichnen. Da zusätzliche Bildinformationen aber ausdrücklich für eine
kurzfristige Verdeckungskompensation erwünscht sind, müssen diese verdeckten
Punkte auch in das Bild übernommen werden.

\section{Der Punktwolkenrenderer}

\section{Systemablauf}

\begin{itemize}
	\item Eingabe von Punktwolke
	\item räumliche Unterteilung der Punktwolke (Octree)
	\item Raytracing durch Octree
	\item ergibt alle depth Pixel auf einem strahl
	\item Kriterium für optimalen Abstand und Anzahl von Tiefenebenen von Framerateziel
	\item Detail Culling und Viewport Culling
	\item eigentliches Rendern geschieht nur durch rendern/morphen des LDI
	\item bei bewegen des Viewport sollte möglichst schnell der LDI als Bild zur Verfügung stehen
	\item Parallel dazu wird neuer optimaler LDI generiert aus Kombination von altem LDI und der Punktwolke
	\item Stellen im alten gemorphten LDI finden, die undersampled sind und mit informationen aus Punktwolke auffüllen?
	\item wieder detail culling an oversampled Stellen
\end{itemize}

\end{document}
