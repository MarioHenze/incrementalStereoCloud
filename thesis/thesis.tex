\documentclass[hyperref, beleg, german]{cgvpub}
% other document types next to bachelorofscience:
% masterofscience
% diplominf
% diplomist
% beleg

%more options (to be appended in the square brackets):
% german....... german version 
% female........ to be used for female endings in german
% bibnum....... numerical reference style
% final............ intended for the final submission
% lof.............. genereate list of figures
% lot.............. generate list of tables
% noproblem.. do not show task
% notoc......... do not generate table of contents
% twoside...... two sided layout

%\usepackage[ngerman]{babel}
\usepackage{enumitem}
\usepackage{verbatim}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usepackage{pgffor}

\usetikzlibrary{calc, intersections}

\author{Mario Henze}
\title{The title of the thesis}
\birthday{19. July 1994}
\placeofbirth{Wernigerode}
\matno{4039602}
\betreuer{Dr.\ B.\ Russig}
\bibfiles{literature}
\problem{
VR environments become more and more important in visualization and engineering
applications. In order to provide a comfortable experience without risk of
motion sickness a rendering framerate of 90fps needs to be ensured. Framerate is
even more important than image quality. The goal of this thesis is the
development of an incremental rendering approach for large point cloud data that
can guarantee the necessary framerate by rendering only a subset of the points
per frame and by accumulation information over several frames.

The specific tasks are:
\begin{itemize}
\item Literature research on point cloud rendering and incremental rendering
	approaches
\item Justified selection of an incremental rendering approach for basic point
	cloud rendering that guarantees a user-defined framerate and converges
	to a good approximation of a non-incremental rendering without framerate
	limitations.
\item Implementation of the selected approach in a plugin to the CGV framework
\item Evaluation of the achieved image quality for stereoscopic rendering at
	90fps with a suitable image quality metric for several point clouds of
	different sizes in comparison to a non-incremental rendering approach.
\end{itemize}

Optional Tasks:
\begin{itemize}
\item Optimization of the rendering approach for stereoscopic render
\item Implementation of an advanced hole-free point cloud rendering approach
\end{itemize}
}
\copyrighterklaerung{If the author used ressources from third parties (texts,
images, code) he or she should state the consents of the copyright owners here
or cite the given general conditions (e.g. CC/ (L) GPL/BSD copyright notices)}
\acknowledgments{I'd like to thank...}
\abstracten{abstract text english}
\abstractde{ abstract text german}
\begin{document}

\chapter{Einleitung}%
\label{sec:einleitung}

Der Prozess der Punktwolkenakquisition ist durch technologische Fortschritte
mittlerweile für eine große Anzahl an Problemstellungen machbar und
kostengünstig. So sind bereits Geräte wie die Xbox Kinect oder Intel
Realsense, die selbst für Privatpersonen erschwinglich sind, in der Lage,
Tiefeninformationen zu erfassen und letztendlich Punktwolken zu generieren. Mit
dem Aufkommen von Head Mounted Displays wie der Oculus Rift oder HTC Vive
existieren nun auch Darstellungsmöglichkeiten, die die gewonnen
Tiefeninformationen der menschlichen Wahrnehmung direkter und intuitiver
zugänglich machen.

Die Bildsynthese und Wiedergabe in solchen HMD Systemen stellt jedoch besondere
Vorgaben in Hinblick auf Benutzbarkeit. So muss jederzeit sichergestellt werden,
dass die Immersion dieser Virtual Reality Umgebung erhalten bleibt. Das bedeutet
es herschen harte Schranken für Eingabeverzögerung und Bildwiederholfrequenz und
deren Übertretung hat den Verlust von Interaktivität und im schlimmsten Fall
physiologische Probleme wie Motion Sickness zur Folge.

In dieser Arbeit soll ein Verfahren zur interaktiven Darstellung von Punktwolken
entwickelt werden, welches in Hinblick auf die genannten Probleme optimiert ist.
So besteht das Ziel darin, selbst größte Datensätze noch in interaktiver Weise
darzustellen. Dazu werden zunächst Ansätze zur klassischen Punktwolken
Darstellung und zum inkrementellen Rendering vorgestellt und deren
Kombinationsmöglichkeiten untersucht. In einem weiteren Schritt soll, das so
entwickelte Verfahren als C++ Plugin für das cgv-Framework des Lehrstuhls
umgesetzt werden.

\chapter{Verwandte Arbeiten}%
\label{sec:verwandte_arbeiten}

\section{Überblick}

Im Bereich des Image-based Rendering kann die Arbeit~\cite{shum2000review} als
Überblick gesehen werden. Heung-Yeung und Sing Bing arbeiten hierbei die
unterschiedliche Verwendung von geometrischen Information bei verschiedenen
Bildsyntheseverfahren heraus. So wird zunächst eine plenoptische Funktion
definiert, welche als Grundlage der geometrielosen Verfahren genutzt wird. In
der Mitte des Spektrums sind Methoden mit impliziter Geometrieinformation
aufgeführt. In diese Kategorie fallen View-morphing und Transfermethoden, welche
die geometrische Redundanz zweier aufeinanderfolgender Bilder ausnutzen. Zuletzt
werden auch die klassischen Verfahren der Bildsynthese, welche auf explizite
Geometrie wie Dreiecksnetze aufsetzen, eingeordnet.

In~\cite{chang1999ldi} stellen Chun-Fa et al.\ basierend auf~\cite{he1998layered}
eine Datenstruktur vor, welche zur Organisation von Referenzbildern mit
variierender Tiefenauflösung genutzt werden können. So werden Bilder aus
verschieden Perspektiven in einen sog. Layered Depth Tree eingfügt. Da bei
gleicher Auflösung Referenzbilder aus einer näheren Perspektive
Tiefeninformationen höher abtasten als entferntere, können diese in einen Octree
eingeordnet werden und es entsteht eine Art Ordnungsrelation. Mithilfe dieser
Datenstruktur ist es nun möglich, die Referenzbilder auswählen, deren
Tiefenauflösung in etwa dem zu synthetisierenden Bildausschnitts entspricht. In
der praktischen Umsetzung zeigen Chun-Fa et al.\ auf, dass der Speicherverbrauch
nicht wesentlich höher als die direkte Speicherung aller Referenzbilder ist und
auch die Rechenzeit im schlimmsten Fall gleich dem direkten Rendern aller
Referenzbilder ist.

Ein generelles Vorgehen beim Warpen von 3D Renderings wird von Mark et al.\
in~\cite{mark1997post} vorgestellt. So wird die These untersucht, dass ein Paar
von gerenderten Bildern und deren entsprechende Tiefenpuffer ausreichen um
beliebige Ansichten aus benachbarten Perspektiven zu zeichnen. Unter
Zuhilfenahme der in~\cite{mcmillan1995head} formulierten Morphing Gleichung
wird eine Korrespondenz zwischen beiden Bildern hergestellt und ein
kombiniertes Bild an einer neuen Position erzeugt. Anschließend werden
verschiedene Probleme des Verfahrens aufgezeigt. So entstehen beim
bildbasierten Morphing partielle und totale Verdeckungsartefakte. Die in Bezug
zum Betrachter variierende diskrete Abtastung von Oberflächen führt weiterhin
zur Entstehung von Löchern. Um diese Effekte auszugleichen werden mehrere
Strategien vorgestellt, welche auf der Rekonstruktion von Geometrie und
optimierter Interpolation basieren.

Mit QSplat wurde ein einflussreiches Verfahren zum Zeichnen von Punktwolken von
Rusinkiewicz und Levoy in~\cite{rusinkiewicz2000qsplat} entwickelt. Hierbei
wird das effiziente Rendern durch die Verwendung einer Hierarchie von
Begrenzungskugeln ermöglicht. In einem Vorverarbeitungsschritt wird zunächst
ein Dreiecksnetz genutzt um bei der Bestimmung der Begrenzungskugeln
Oberflächennormalen einzubeziehen und Lochartefakte unterbinden zu können.
Anschließend kann die Hierarchie durch eine rekursive Durchquerung gezeichnet
werden. Dabei werden alle Pfade bei Verfehlen eines Sichtbarkeitskriteriums
verworfen. Des Weiteren werden der Einfluss von verschiedenen Splatting Formen
auf die Bildqualität untersucht und die Laufzeit von Vorverarbeitung und
Rendering aufgeschlüsselt.

Goswami et al.\ benutzen ein variablen Detailgrad beim Zeichnen und eine
Baumstruktur zur Verwaltung der Punktwolke~\cite{goswami2010high}. Grundlegend
muss bei diesem Verfahren zunächst eine Vereinfachung und Zusammenfassung der
Punkte erfolgen. Dafür werden verschieden große Cluster in Abhängigkeit der
lokalen Varianz erzeugt und in einen KD Baum eingefügt. Eine Quantisierung des
Datenraums entlang der Koordinatenachsen ermöglicht weiterhin eine schnelle
Umsetzung dieses Prozesses, da jede Zelle parallel bearbeitet werden kann. Für
das Rendering werden alle Knoten dieses LOD-Baums in eine
Prioritätswarteschlange nach ihrem Detailgrad eingeordnet. Beginnend vom
Wurzelknoten werden solange die Cluster expandiert, bis eine ausreichende
Punktdichte erreicht ist.

Einen weitere Implementation mit Fokus auf Virtual Reality wird von Discher et
al.\ in~\cite{discher2018point} vorgestellt. Das Punktwolkenrenderering wird
dabei in drei Phase eingeteilt. Zunächst wird eine sog.\ repräsentative
Untermenge in Abhängigkeit der Auslastung von CPU und GPU ausgewählt. Nach dem
Entfernen von Punkten außerhalb des Sichtkegelstumpfes und überflüssigen
Details wird ein KD Baum aufgebaut. Das Renderen erfolgt dann in G-Buffer
(siehe~\cite{saito1990comprehensible}), welche bestimmte geometrische
Eigenschaften wie Tiefe oder Oberflächennormalen als Zwischenbild abspeichern.
Die Anwendung dieser Zwischentexturen ermöglicht abschließend eine
Bildnachbearbeitung. Dieser finale Schritt dient dazu die Bildqualität
nachträglich zu steigern. Zum Beispiel können Löcher gefüllt, Aliasing
Artefakte beseitigt und Kanten besonders hervorgehoben werden. Außerdem wurden
mehrere Kompositionsansätze für die anschließende Bildsynthese untersucht und
deren Laufzeitperformance evaluiert.

Wimmer und Scheiblauer entwickeln in~\cite{wimmer2006instant} ein Verfahren,
dass ohne Vorverarbeitung eine möglichst große Menge an Punkten mit
interaktiven Bildraten darstellen soll. Zur Umsetzung werden memory optimized
sequential point trees (MOSPT) und verschachtelte Octrees als Datenstrukturen
vorgestellt. Dies ermöglicht ein serielles Rendern der Punktwolke nach dem out
of core Prinzip. Um eine lokale Zusammenfassung von Punkten und damit eine
Datenreduktion zu erreichen wird eine Fehlermetrik eingeführt die zur
Sortierung der MOSPT genutzt wird. Die so gewonnene Hierarchie ermöglicht nun
das Verwerfen von Knoten mit zu geringem Einfluss auf das resultierende Bild.
Für den Rendervorgang werden zunächst ein Intervall auf der Fehlermetrik
bestimmt und alle entsprechenden Knoten des MOSPT gezeichnet. Da die Nutzung
eines einzelnen MOSPT statische Vorraussetzungen wie ein globales LOD
vorraussetzen wird nur ein Teil der Punktwolken in diesen angeordnet. Diese
Teilmengen werden wiederum in einen Octree eingefügt. Abschließend wird der
Laufzeitvorteil der MOSPT untersucht und auf Artefakte des Renderingprozesses
eingegangen.

\section{Motivation des Renderingansatzes}

Wie bereits in Abschnitt~\ref{sec:verwandte_arbeiten} aufgezeigt, existieren
eine Vielzahl an Renderverfahren für Punktwolken und bildbasierten Morphing
Ansätzen. Das Ziel dieser Arbeit besteht darin eine Kombination dieser beiden
Vorgehen zu entwickeln, welche sowohl ein schnelles Zeichnen von
unverarbeiteten Punktwolken als auch eine Wiederverwendung von bereits
gezeichneten Bildbereichen ermöglicht.

Zunächst ist hervorzuheben, dass der Großteil der Punktzeichnungs Algorithmen
eine Vorverarbeitung der Punktwolke vorrausetzt. So ist zu erkennen, dass
generell Artefakte der Punktwolkenakquisition ausgeglichen und eine
Speicherzugriff optimierende Datenstruktur erreicht werden soll. Hierfür wird
fast ausschließlich die räumlich variierende Punktdichte zum Clustering
genutzt. Anschließend wird eine Baumstruktur Hierarchie aufgebaut, welche das
rekursive Zeichnen bis zu einem ausreichenden Detailgrad ermöglicht. Da die
vorliegende Arbeit jedoch darauf abzielt zu einem späteren Zeitpunkt
kontinuierlich erfasste Punkte in die Darstellung einfließen zu lassen, ist ein
aufwendiges Verarbeiten nicht möglich. So liegt der Zeitaufwand von
QSplat~\cite{rusinkiewicz2000qsplat} beim Preprocessing einer Wolke mit rund
127 Millionen Punkten bei einer knappen Stunde. Selbst modernere Ansätze
wie~\cite{goswami2010high} benötigen für diesen Verarbeitungsschritt immer noch
mehrere Minuten. Auch wenn diese Angaben aufgrund von unterschiedlicher
Hardware nicht direkt vergleichbar sind, geben sie eine Größenordnung der zu
erwartenden Laufzeitkosten an.

Betrachtet man die Problemstellungen jedoch aus Sicht des bildbasierten
Morphings, verspricht die Umsetzung des Renderingprozesses als Layered Depth
Image analog zu~\cite{chang1999ldi} einen Vorteil. So werden in dem LDI
Bildpunkte mit verschiedenen Tiefen gespeichert. Außerdem ist eine elegante
Interpolation zwischen verschiedenen Referenzen durch die Morphing Gleichung
von McMillan (siehe~\cite{mcmillan1995list} und~\cite{mcmillan1995plenoptic})
möglich. Unter der Annahmen, dass die globalen Punkte der Wolke äquivalent zu
den relativen Punkten des LDIs sind, kann das Rendern von Punktwolken in ein
Rendern von LDIs überführt werden. Das bedeutet, der einzige
Vorverarbeitungsschritt der Punktdaten besteht darin, die Punkte aus dem
globalen Koordinatensystem in das kamerabezogene System zu transformieren.

Außerdem ist festzustellen, dass bei der Anwendung von HMDs eine ständige
Veränderung der Kameraposition durch die teilweise willkürlichen Bewegungen des
menschlichen Kopfes auftreten. Somit sind Renderverfahren von Vorteil, die eine
schnelle Interpolation von benachbarten Ansichten ermöglichen. Da auch dieses
Kriterium durch die LDIs abgedeckt wird, baut die folgende Implementation auf
Basis dieser einen eigenen Rendervorgang auf.

\chapter{Theoretische Betrachtungen}

\section{Plenoptic Modelling}

Um neuartige Renderansätze zu entwicklen, ist es hilfreich diese in nach einer
Top-Down Methodik zu entwickeln. Dafür bietet sich die plenoptische Modellierung
an. Hierbei wird zunächst versucht, die Szene unabhängig von Beobachtern zu
beschreiben. Adelson und Bergen definieren dafür in~\cite{adelson1991plenoptic}
die plenoptische Funktion wie folgt:

\begin{equation}
	P_7 = P(V_x, V_y, V_z, \theta, \Phi, \lambda, t)
\end{equation}

\begin{description}[style=sameline]
	\item[\( V_x, V_y, V_z \)] mögliche Position der Kamera
	\item[\( \theta, \Phi \)] Winkel eines Sichtstrahls
	\item[\( \lambda \)] Wellenlänge des Lichts
	\item[\( t \)] Zeitpunkt der Beobachtung
\end{description}

Diese Form der Beschreibung ermöglicht es die Szene zu jedem Zeitpunkt an jeder
Stelle und in jede Richtung zu samplen. Anhand dieses generellen Modells können
nun Vereinfachungen vorgenommen werden.

Es ist zunächst festzustellen, dass Punktwolkendaten in einem globalen
Koordinatensystem gegeben sind. Beim Punktwolkenrendering gehen jedoch nur die
sichtbaren Punkte in das Bild ein. Außerdem erfolgt die Kamerabewegung von HMDs
ausschließlich kontinuierlich. Daraus folgt, dass sich die Untermengen alles
sichtbaren Punkte zwischen zwei gerenderten Ansichten nahezu identisch sind.
Ähnliche Annahmen liegen auch dem Modell der Layered Depth Images zugrunde.

McMillan und Bishop ermöglichten in~\cite{mcmillan1995plenoptic} unter
Ausschluss von \( t \) und \( \lambda \) das plenoptische Modellieren. In
diesem Kontext kann ein gewöhnliches Bild nun als unvollständige Abtastung der
plenoptischen Funktion an einem festen Betrachtungspunkt aufgefasst werden.

\section{Layered Depth Image}

\begin{figure}
	\centering
	\tdplotsetmaincoords{70}{110}
	\input{figures/layereddepthimage.tikz}
	\caption{Veranschaulichung von Punkten in einem LDI. Alle Punkte mit
	gleicher Pixelzugehörigkeit werden zusammenhängend gespeichert. Jeder
	Punkt im Pixel besitzt eine Tiefe \(z\) und eine Farbe \(c\).}%
	\label{fig:layereddepthimage}
\end{figure}

\begin{figure}
	\centering
	\tdplotsetmaincoords{70}{60}
	\input{figures/ldicoordinatesystem.tikz}
	\caption{Zusammenhänge im LDI Koordinatensystem. Die Vektoren \(i\),
	\(j\) und \(k\) spannen das Weltkoordinatensystem auf. Die Vektoren
	\(a\), \(b\) und \(c\) sind die Basis LDI Bildraums. (Darstellung 
	adaptiert aus~\cite{mcmillan1997image})}%
	\label{fig:ldicoord}
\end{figure}

\begin{figure}
	\centering
	\input{figures/planemorph.tikz}
	\caption{Morphing von zwei Kamerapositionen. (Darstellung adaptiert
	aus~\cite{mcmillan1997image})}%
	\label{fig:ldimorph}
\end{figure}

\begin{itemize}
	\item Von kamera position
	\item x- und y-Auflösung als raytracing
	\item layered depth pixel ist Farbwert mit Tiefeninformation in Abhängigkeit der
	      Kameraposition
	\item neuer Viewport über Matrixtransformation
\end{itemize}

\chapter{Konzeption}

Die Entkopplung und Nebenläufigkeit der Abläufe in der Implementation sind von
großer Wichtigkeit. So können folgende unabhängige Prozesse definiert werden.

Zunächst ist eine Datenverarbeitung notwendig. In Abhängigkeit des
Datenformates und der Größe der Punktwolkendatei müssen die Punkte als Ganzes
oder als Teilmenge im Arbeitsspeicher vorgehalten werden. Des Weiteren sollten
initiale Koordinatentransformation in diesem Prozess durchgeführt werden.

Einen weiteren Prozess stellt das Rendering der Punktwolke dar. Um eine
konstante Framerate zu erreichen muss der Arbeitsaufwand möglichst konstant
gehalten werden. Deshalb werden die Punkte in einem LDI vorgehalten. Diese
Datenstruktur ermöglicht eine triviale Rasterrisierung und die Anwendung der
Morphing Equation.

Da bei großen Datenwolken Speicherknappheit auftreten würde, muss eine
Selektion aus dem gesamten Datensatz geschehen. Unter der Annahme, dass nur ein
Teil der Punkte im Sichtfeld des Betrachters für eine qualitative
Repräsentation der Daten ausreicht kann der letzte Prozess deklariert werden.
Hierfür wird das LDI des Renderingprozesses als der Kegelstumpf der
Betrachtungstransformation interpretiert. Die Vereinigung dieses Kegelstumpfes
und dem Koordinatenraum der Punktwolke schränkt die Menge der zu verarbeitenden
Punkte initial ein. Da das LDI lediglich Daten in Bezug auf den Betrachter
speichert, würden alle außerhalb liegenden Punkte nicht in der Rasterisierung
auftreten und können somit verworfen werden. Wenn sich die
Betrachtungstransformation geändert hat, muss außerdem ein neues LDI erzeugt
werden. Unter der Vorraussetzung, dass die Differenz der beiden Betrachtungen
möglichst gering sind, können die Punkte des alten LDIs übernommen werden.
Hierbei besitzen zwei Kegelstümpfe eine geringen Unterschied, wenn sich deren
vereinigtes Volumen nicht stark vom Volumen eines einzigen Keglstumpfes
unterscheidet. Eine Translation um ein Viertel der Bildschirmbreite oder eine
Rotation entlang der Blickrichtungsachse sind Beispiele für einen geringen
Unterschied zwischen zwei Betrachtern. In diesem Fall ist das Morphing mit
seinem geringen linearen Aufwand besonders lohnenswert, da bestenfalls der
Großteil der neuen Perspektive mit bereits transformierten Daten abgedeckt
werden kann. Es werden sich aber immer Stellen im LDI ergeben, an denen keine
Daten vorhanden sind. Diese Ausschnitte müssen deshalb erkannt und mit Daten
der ursprünglichen Punktwolke gefüllt werden. Hierfür muss nach dem Morphen die
lokale Punktdichte im LDI ermittelt werden. Ziel dieser Berechnung ist es
möglichst zusammenhängende Pixelbereiche zu erkennen. Sind diese Bereiche
können diese als Anfrage an den ursprünglichen Datensatz gestellt werden.

Durch die oben beschriebene Trennung ist eine Implementation möglich, welche
aus Sicht des Renderings besonders gleichmäßige Frametimes ermöglicht. So
stellt die Rasterisierung der Punkte im Sichfeld den einzigen zeitkritischen
Pfad im System dar, welcher direkt auf die Qualität der Darstellung Einfluss
nimmt. Die Datenverarbeitung der ursprünglichen Punktwolke in ein geeignetes
Format und die Bearbeitung der LDIs sind sekundäre Faktoren. Das heißt die Rate
mit der neue Punkte nachgeladen werden können, verbessert die Detailwiedergabe,
ist für die prinzipielle Funktion jedoch beliebig.

\chapter{Implementation}

Die vorliegende Arbeit implementiert den oben beschriebenen Ansatz eines LDIs.
Als Grundlage für die Benutzeroberfläche und die Interaktion mit der
Grafikhardware wird das CGV Framework des Lehrstuhls verwendet. Um möglichst
viele Punktwolkenformate als Eingabe nutzen zu können wird weiterhin die
bestehende Klasse \texttt{point\_cloud} genutzt.

\section{Grundlegende Abstraktionen}

\begin{figure}
	\centering
	\tdplotsetmaincoords{70}{110}
	\input{figures/pinholecamera.tikz}
	\caption{Abbildungsverhältnisse einer Lochkamera auf Sensor- und reale
		Bildebene}%
	\label{fig:pinholecamera}
\end{figure}

Da LDIs prinzipbedingt auf den Betrachter bezogen definiert sind, ist die
Klassendeklaration \texttt{PinholeCameraModel} nützlich. Hierbei wird die in
der Literatur häufig getroffene Vereinfachung einer Lochkameratransformation
nachgebildet. So kann die Lochkamera als idealisierte optische Kamera
aufgefasst werden, welche alle Sichtstrahlen in aus der Szene in einem einzigen
Punkt bündelt (siehe Abbildung~\ref{fig:pinholecamera}). Bei praktischen
Aufbauten eine Lochkamera wird das projizierte Bild hinter diesem Brennpunkt
gespiegelt auf einer Ebene abgebildet. Zur Vereinfachung wird auf diesen
Umstand verzichtet und die Bildebene kurz vor dem Brennpunkt zwischen Szene und
Koordinatenursprung der Kamera platziert. Dadurch wird auch die Spieglung der
Abbildung verhindert und die geometrischen Zusammenhänge können mit einfachen
Strahlensätzen interpretiert werden. In Anlehnung an digitale Linsenkameras
wird diese Ebene im Quellcode auch als Sensorebene bezeichnet. Des Weiteren
definiert die Lochkamera mit ihrem Öffnungswinkel und ihrem Seitenverhältnis
einen Sichtkegelstumpf. Die \texttt{PinholeCameraModel} Klasse vereint diese
beiden Anwendungen, indem es die Kamera- und Perspektivtransformation als
Membervariable speichert und in geeigneter Weise über Zugriffmethoden bereit
stellt. So kann der Sichtkegelstumpf eines LDIs zur Sichtbarkeitsprüfung von
Punkten genutzt werden. Durch die Rasterisierung der Punkte im LDI ist es
außerdem notwendig die Pixelauflösung der Sensorebene und damit auch der
Lochkamera zu speichern. Dies geschieht durch eine weitere Membervariable vom
Typ \texttt{std::pair<size\_t, size\_t>}.

\section{Die Punktwolke und Punktwolkenanfragen}

Die Punktwolke wird durch die \texttt{PointCloudSource} abstrahiert. Die
konkreten Punktdaten werden als Membervariable vom Typ \texttt{point\_cloud}
gekapselt. Durch diesen ist es möglich getrennt auf Koordinaten und andere
Punktmerkmale wie Farbe oder Normalen zuzugreifen.

Außerdem verwaltet die \texttt{PointCloudSource} eine Liste aller gestellten
\texttt{PointCloudQuery}s in einer weiteren Membervariable. Diese Anfragen
speichern als Member zunächst die Kamerakonfiguration vom Typ
\texttt{PinholeCameraModel}. Mit dieser ist es möglich, die Punkte der gesamten
Punktwolke nach Sichtbarkeit zu filtern. Die \texttt{PointCloudQuery} besitzt
zwei eigene Membervariablen zum Speichern der Punktpositionen und Farben. Nach
der Sichtbarkeitsprüfung werden alle verbliebenen Punkte deshalb in die Anfrage
kopiert. Da die Anfrageklasse außerdem den Schnittpunkt zwischen
Dateiverarbeitung und Rendering darstellt ist eine weitere Membervariable zur
Sicherung von kritischen Abschnitten notwendig. Die Mutexvariablen vom Typ
\texttt{std::timed\_mutex} besitzen die Besonderheit eine vorher definierte
Wartezeit zu berücksichtigen. Dies ist aus Sicht des Renderings notwendig um
eine Obergrenze für die Frametimes einhalten zu können.

Um den Verwaltungsaufwand gering zu halten, durlaufen alle Anfragen drei
Zustände entsprechend ihrer Bearbeitung. Anfangs befinden sich die Anfragen in
einem ``unvollständigen'' Zustand. Dieser bedeutet, dass noch nicht alle Punkte
aus der Datenverarbeitung vorliegen. Nach dem vollständigen Kopieren aller
sichtbaren Punkte geht die Anfrage in einem ``vollständigen'' und
``konsumierbarem'' Zustand über. Daraufhin filtert der Renderingprozess die
Liste aller Anfragen nach der Konsumierbarkeit und fügt die entsprechenden
Punkte ein. So wird während des Renderings vesucht im vorgegebenen Zeitbudget
so viele Punkte wie möglich in das LDI zu transformieren. Anschließend ist der
Zweck der Anfrage erfüllt und sie kann aus der Liste entfernt werden. Um
Wettlauf-Risiken bei den obigen Zustandsänderungen auszuschließen, werden zwei
Membervariablen des Typs \texttt{std::atomic\_bool} verwendet. Deren Umsetzung
garantiert auf x86 basierten Computern einen wohldefinierten Zustand zu allen
Zeitpunkten der nebenläufigen Benutzung. Außerdem kann hierdurch auf
laufzeittechnisch teure Mutexvariablen und deren fehleranfällige Verwaltung
verzichtet werden.

\section{Das Layered Depth Image}

Das Layered Depth Image stellt die zentrale Datenstruktur der Implementation
dar. Sie kann als eine um einen Datenspeicher erweiterte Lochkamera
interpretiert werden. So ist bereits die \texttt{PinholeCameraModel} Klasse mit
rasterisierten Koordinaten ausgestattet. Da diese Pixelauflösung die Grenze der
Positionsauflösung auf der Sensorebene darstellt, ergeben sich folgende
Festlegungen. Nach der entsprechenden Perspektivtransformation befinden sich
alle sichtbaren Punkte an einer diskreten \( (x, y) \) Position auf der
Sensorebene. Zusätzlich ist deren Tiefe in Bezug auf die Kamera von Bedeutung.
Werden räumlich verschiedene Punkte auf die selbe \( (x,y) \) Position
transformiert, unterscheidet sich deren Tiefenwert \(z\). Konventionelle
Rasterrisierungsansätze würden bei diesem Ansatz den Tiefenpuffer zur
Konfliktauflösung verwenden und den der Kamera näheren Punkt in das Bild
zeichnen. Da zusätzliche Bildinformationen aber ausdrücklich für eine
kurzfristige Verdeckungskompensation erwünscht sind, müssen diese verdeckten
Punkte auch in das Bild übernommen werden. Hierfür wird die \texttt{ray\_t}
Struktur definiert. Es handelt sich hierbei um einen \texttt{std::vector} der
für jeden Punkt dessen Tiefenwert und Farbe speichert. Die \( (x,y) \) Position
ist indirekt durch die Zugehörigkeit zum jeweiligen Strahlvektor gegeben. Wie
an der Benennung zu erkennen, kann jede dieser Strukturen als ein Strahl vom
Brennpunkt durch den jeweiligen Pixel des Sensors gesehen werden. Parallelen
zum Raytracing ergeben sich nur aus der Art und Weise der Datenerfassung. Das
eigentliche Zeichnen geschieht nach dem klassischen Rasterisierungsansatz. Die
Gesamtheit aller Pixel des LDI werden wiederum gespeichert als
\texttt{std::vector<ray\_t>}. Hierbei ist zu beachten, dass der lineare Zugriff
durch den Array Subscript Operator zusammen mit einer Hilfsfunktion erfolgt,
die zweidimensionale Koordinaten entsprechend umrechnet.

Um den zentralen Vorteil der linearen Korrespondenz zwischen zwei LDIs zu
nutzen, wird die \texttt{warp\_reference\_into} Memberfunktion deklariert.
Diese wendet die McMillan Morphing Gleichung auf das Argument LDI an und
übernimmt alle weiterhin sichtbaren Punkte in sich selbst. Des Weiteren können
Punkte aus dem globalen oder dem kamerabezogenen Koordinatensystem mit
\texttt{add\_global\_points} bzw. \texttt{add\_transformed\_points} eingfügt
werden.

\section{Der Punktwolkenrenderer}

Der \texttt{FastPointCloudRenderer} setzt die oben aufgezählten
Funktionalitäten zusammen, indem er den aktiven Viewport bzw. Framebuffer des
CGV-Viewers als LDI auffasst. Um diese Interoperabilität zu ermöglichen, leitet
die Renderklasse zunächst von \texttt{cgv::render::drawable} ab und
implementiert die entsprechenden Funktionen des klassischen Renderingablaufs.
So wird für den Drawcall der Referenzpunktrenderer des Frameworks genutzt, der
zuvor mit den Daten des LDIs versorgt wurde.

Nach einem erfolgten Drawcall wird von der Punktwolken Membervariable eine
abgeschlossene Anfrage gefordert und die enthaltenen Punkte in das Viewport LDI
eingfügt. Dies bedeutet implizit, dass die entsprechenden Daten durch den
Referenzpunktrenderer auf den Speicher der GPU geladen werden. Hierfür muss
zunächst ein kritischer Abschnitt begonnen werden. Innerhalb diesem können nun
die Punkte aus der Punktwolkenanfrage in ein neues LDI übernommen werden.
Zusätzlich wird das alte LDI in die neue Ansicht gemorpht. Abschließend wird die
Membervariable \texttt{m\_ldi} durch das eben erstellte LDI ersetzt und der
kritische Abschnitt kann wieder verlassen werden.

Da die Klasse \texttt{FastPointCloudRenderer} ein zentraler Interaktionspunkt
zwischen CGV Viewer und allen anderen Bestandteilen der Implementation ist, wird
die beschriebene Nebenläufigkeit durch Threads als Membervariablen umgesetzt.
Deren Funktionalität wird definiert durch Zuweisung eines Lambda Ausdrucks in
der \texttt{init()} Methode. Hierdurch wird eine Trennung zwischen
Funktionalität und der Art und Weise der Ausführung erreicht. Somit sind
Anpassung an neuartige Parallelitätsmuster wie Coroutines einfacher möglich.

\section{Systemablauf}

\begin{itemize}
	\item Eingabe von Punktwolke
	\item räumliche Unterteilung der Punktwolke (Octree)
	\item Raytracing durch Octree
	\item ergibt alle depth Pixel auf einem strahl
	\item Kriterium für optimalen Abstand und Anzahl von Tiefenebenen von Framerateziel
	\item Detail Culling und Viewport Culling
	\item eigentliches Rendern geschieht nur durch rendern/morphen des LDI
	\item bei bewegen des Viewport sollte möglichst schnell der LDI als Bild zur Verfügung stehen
	\item Parallel dazu wird neuer optimaler LDI generiert aus Kombination von altem LDI und der Punktwolke
	\item Stellen im alten gemorphten LDI finden, die undersampled sind und mit informationen aus Punktwolke auffüllen?
	\item wieder detail culling an oversampled Stellen
\end{itemize}

\end{document}
