\documentclass[hyperref, beleg, german]{cgvpub}
% other document types next to bachelorofscience:
% masterofscience
% diplominf
% diplomist
% beleg

%more options (to be appended in the square brackets):
% german....... german version 
% female........ to be used for female endings in german
% bibnum....... numerical reference style
% final............ intended for the final submission
% lof.............. genereate list of figures
% lot.............. generate list of tables
% noproblem.. do not show task
% notoc......... do not generate table of contents
% twoside...... two sided layout


\author{Mario Henze}
\title{The title of the thesis}
\birthday{19. July 1994}
\placeofbirth{Wernigerode}
\matno{4039602}
\betreuer{Dr. B. Russig}
\bibfiles{literature}
\problem{
VR environments become more and more important in visualization and engineering
applications. In order to provide a comfortable experience without risk of
motion sickness a rendering framerate of 90fps needs to be ensured. Framerate is
even more important than image quality. The goal of this thesis is the
development of an incremental rendering approach for large point cloud data that
can guarantee the necessary framerate by rendering only a subset of the points
per frame and by accumulation information over several frames.

The specific tasks are:
\begin{itemize}
\item Literature research on point cloud rendering and incremental rendering
	approaches
\item Justified selection of an incremental rendering approach for basic point
	cloud rendering that guarantees a user-defined framerate and converges
	to a good approximation of a non-incremental rendering without framerate
	limitations.
\item Implementation of the selected approach in a plugin to the CGV framework
\item Evaluation of the achieved image quality for stereoscopic rendering at
	90fps with a suitable image quality metric for several point clouds of
	different sizes in comparison to a non-incremental rendering approach.
\end{itemize}

Optional Tasks:
\begin{itemize}
\item Optimization of the rendering approach for stereoscopic render
\item Implementation of an advanced hole-free point cloud rendering approach
\end{itemize}
}
\copyrighterklaerung{If the author used ressources from third parties (texts,
images, code) he or she should state the consents of the copyright owners here
or cite the given general conditions (e.g. CC/(L)GPL/BSD copyright notices)}
\acknowledgments{I'd like to thank...}
\abstracten{abstract text english}
\abstractde{ abstract text german}
\begin{document}

\section{Literaturübersicht}

Dieser Abschnitt dient zunächst der Auflistung potentieller Quellen und wird in
der finalen Fassung nicht vorkommen.

\begin{description}
	\item[\cite{discher2018point}]
	\item[\cite{rusinkiewicz2000qsplat}]
	\item[\cite{goswami2010high}]
	\item[\cite{wimmer2006instant}]
	\item[\cite{shum2000review}]
	\item[\cite{mark1997post}]
	\item[\cite{mcmillan1995head}]
	\item[\cite{chang1999ldi}]
	\item[\cite{he1998layered}]
	\item[\cite{mcmillan2009image}]
\end{description}

\section{Einleitung}
\label{sec:einleitung}

Der Prozess der Punktwolkenakquisition ist durch technologische Fortschritte
mittlerweile für eine große Anzahl an Problemstellungen machbar und
kostengünstig. So sind bereits Geräte wie die Xbox Kinect oder Intel
Realsense, die selbst für Privatpersonen erschwinglich sind, in der Lage,
Tiefeninformationen zu erfassen und letztendlich Punktwolken zu generieren. Mit
dem Aufkommen von Head Mounted Displays wie der Oculus Rift oder HTC Vive
existieren nun auch Darstellungsmöglichkeiten, die die gewonnen
Tiefeninformationen der menschlichen Wahrnehmung direkter und intuitiver
zugänglich machen.

Die Bildsynthese und Wiedergabe in solchen HMD Systemen stellt jedoch besondere
Vorgaben in Hinblick auf Benutzbarkeit. So muss jederzeit sichergestellt werden,
dass die Immersion dieser Virtual Reality Umgebung erhalten bleibt. Das bedeutet
es herschen harte Schranken für Eingabeverzögerung und Bildwiederholfrequenz und
deren Übertretung hat den Verlust von Interaktivität und im schlimmsten Fall
physiologische Probleme wie Motion Sickness zur Folge.

In dieser Arbeit soll ein Verfahren zur interaktiven Darstellung von Punktwolken
entwickelt werden, welches in Hinblick auf die genannten Probleme optimiert ist.
So besteht das Ziel darin, selbst größte Datensätze noch in interaktiver Weise
darzustellen. Dazu werden zunächst Ansätze zur klassischen Punktwolken
Darstellung und zum inkrementellen Rendering vorgestellt und deren
Kombinationsmöglichkeiten untersucht. In einem weiteren Schritt soll, das so
entwickelte Verfahren als C++ Plugin für das cgv-Framework des Lehrstuhls
umgesetzt werden.

\section{Verwandte Arbeiten}
\label{sec:verwandte_arbeiten}

Im Bereich des Image-based Rendering kann die Arbeit \cite{shum2000review} als
Überblick gesehen werden. Heung-Yeung und Sing Bing arbeiten hierbei die
unterschiedliche Verwendung von geometrischen Information bei verschiedenen
Bildsyntheseverfahren heraus. So wird zunächst eine plenoptische Funktion
definiert, welche als Grundlage der geometrielosen Verfahren genutzt wird. In
der Mitte des Spektrums sind Methoden mit impliziter Geometrieinformation
aufgeführt. In diese Kategorie fallen View-morphing und Transfermethoden, welche
die geometrische Redundanz zweier aufeinanderfolgender Bilder ausnutzen. Zuletzt
werden auch die klassischen Verfahren der Bildsynthese, welche auf explizite
Geometrie wie Dreiecksnetze aufsetzen, eingeordnet.

In \cite{chang1999ldi} stellen Chun-Fa et al. basierend auf \cite{he1998layered}
eine Datenstruktur vor, welche zur Organisation von Referenzbildern mit
variierender Tiefenauflösung genutzt werden können. So werden Bilder aus
verschieden Perspektiven in einen sog. Layered Depth Tree eingfügt. Da bei
gleicher Auflösung Referenzbilder aus einer näheren Perspektive
Tiefeninformationen höher abtasten als entferntere, können diese in einen Octree
eingeordnet werden und es entsteht eine Art Ordnungsrelation. Mithilfe dieser
Datenstruktur ist es nun möglich, die Referenzbilder auswählen, deren
Tiefenauflösung in etwa dem zu synthetisierenden Bildausschnitts entspricht. In
der praktischen Umsetzung zeigen Chun-Fa et al. auf, dass der Speicherverbrauch
nicht wesentlich höher als die direkte Speicherung aller Referenzbilder ist und
auch die Rechenzeit im schlimmsten Fall gleich dem direkten Rendern aller
Referenzbilder ist.

\section{Plenoptic Modelling}

Um neuartige Renderansätze zu entwicklen, ist es hilfreich diese in nach einer
Top-Down Methodik zu entwickeln. Dafür bietet sich die plenoptische Modellierung
an. Hierbei wird zunächst versucht, die Szene unabhängig von Beobachtern zu
beschreiben. Adelson und Bergen definieren dafür in \cite{adelson1991plenoptic}
die plenoptische Funktion wie folgt:

\begin{equation}
	P_7 = P(V_x, V_y, V_z, \theta, \Phi, \lambda, t)
\end{equation}

\begin{itemize}
	\item[\( V_x, V_y, V_z \)] mögliche Position der Kamera
	\item[\( \theta, \Phi\)] Winkel eines Sichtstrahls
	\item[\( \lambda \)] Wellenlänge des Lichts
	\item[\( t \)] Zeitpunkt der Beobachtung
\end{itemize}

Diese Form der Beschreibung ermöglicht es die Szene zu jedem Zeitpunkt an jeder
Stelle und in jede Richtung zu samplen. Anhand dieses generellen Modells können
nun Vereinfachungen vorgenommen werden.

Es ist zunächst festzustellen, dass Punktwolkendaten in einem globalen
Koordinatensystem gegeben sind. Beim Punktwolkenrendering gehen jedoch nur die
sichtbaren Punkte in das Bild ein. Außerdem erfolgt die Kamerabewegung von HMDs
ausschließlich kontinuierlich. Daraus folgt, dass sich die Untermengen alles
sichtbaren Punkte zwischen zwei gerenderten Ansichten nahezu identisch sind.
Ähnliche Annahmen liegen auch dem Modell der Layered Depth Images zugrunde.

McMillan und Bishop ermöglichten in \cite{mcmillan1995plenoptic} unter
Ausschluss von \(t\) und \(\lambda\) das plenoptische Modellieren. In diesem
Kontext kann ein gewöhnliches Bild nun als unvollständige Abtastung der
plenoptischen Funktion an einem festen Betrachtungspunkt aufgefasst werden.

\section{Layered Depth Image}

\begin{itemize}
\item Von kamera position
\item x- und y-Auflösung als raytracing
\item layered depth pixel ist Farbwert mit Tiefeninformation in Abhängigkeit der
Kameraposition
\item neuer Viewport über Matrixtransformation
\end{itemize}

\section{Systemablauf}

\begin{itemize}
\item Eingabe von Punktwolke
\item räumliche Unterteilung der Punktwolke (Octree)
\item Raytracing durch Octree
\item ergibt alle depth Pixel auf einem strahl
\item Kriterium für optimalen Abstand und Anzahl von Tiefenebenen von Framerateziel
\item Detail Culling und Viewport Culling
\item eigentliches Rendern geschieht nur durch rendern/morphen des LDI
\item bei bewegen des Viewport sollte möglichst schnell der LDI als Bild zur Verfügung stehen
\item Parallel dazu wird neuer optimaler LDI generiert aus Kombination von altem LDI und der Punktwolke
\item Stellen im alten gemorphten LDI finden, die undersampled sind und mit informationen aus Punktwolke auffüllen?
\item wieder detail culling an oversampled Stellen
\end{itemize}

\end{document}
